# Aqua-AI Technical Documentation

**Author:** Architecture & Systems Engineering Team  
**Date:** February 14, 2026

---

## 1. Executive Summary

Aqua-AI is a comprehensive water quality monitoring platform designed to provide real-time environmental intelligence for India. The system integrates official government data, community reports, and AI-powered predictive analytics to offer actionable insights into water quality across the nation.

### Key Capabilities

- **Real-time Monitoring:** Interactive map-based visualization of water quality indicators (WQI) across Indian states and river bodies.
- **AI Forecasting:** Machine learning models (Random Forest, Gradient Boosting) utilizing historical data to predict pollution events and risk levels.
- **Data Integration:** Automated pipelines fetching data from CPCB (Central Pollution Control Board), Ministry of Jal Shakti, and Open Government Data portals.
- **Public Engagement:** Community reporting tools and public health alerts.

---

## 2. System Architecture

### High-Level Design

The Aqua-AI platform follows a modern microservices-inspired layered architecture, containerized for scalability and consistency.

- **Frontend Layer (Presentation):** A React 18 Single Page Application (SPA) serving as the user interface. It handles data visualization, map interactions via Leaflet, and user management.
- **API Gateway & Backend Layer (Application):** A Node.js and Express server that acts as the central orchestration layer. It exposes RESTful endpoints, manages authentication (JWT), and coordinates data retrieval.
- **Data Layer (Persistence):** PostgreSQL optimized with PostGIS for spatial queries, storing user data, sensor readings, and geographical information. Redis is utilized for caching high-frequency read operations.
- **Intelligence Layer (AI/ML):** A dedicated Python-based pipeline for training and serving machine learning models. It processes historical data to generate risk predictions.
- **Integration Layer (ETL):** Python scripts for extracting, transforming, and loading (ETL) data from external government APIs into the central database.

### Component Interaction Flow

1. **Data Ingestion:** The Data Pipeline fetches raw data from government sources, cleanses it, and populates the `water_quality_readings` table.
2. **Analysis:** The AI Engine runs periodically, reading new data, generating risk predictions, and storing them in `ai_predictions`.
3. **User Request:** A user accesses the Frontend Dashboard. The React app sends a request to the Node.js Backend API.
4. **Data Retrieval:** The Backend queries the PostgreSQL database (potentially via Redis cache) for location-specific data and associated predictions.
5. **Response:** Structured JSON data is returned to the Frontend, which renders the interactive map and charts.

---

## 3. Technology Stack

### Frontend

- **Framework:** React 18 with TypeScript
- **Build Tool:** Vite
- **State Management:** React Context API, TanStack Query
- **Styling:** Tailwind CSS, Radix UI (Primitives)
- **Mapping:** Leaflet, React-Leaflet
- **Visualization:** Recharts

### Backend

- **Runtime:** Node.js
- **Framework:** Express.js
- **Database ORM/Query Builder:** Knex.js
- **Authentication:** JSON Web Tokens (JWT), Bcrypt
- **Validation:** express-validator
- **Logging:** Winston

### Data & AI Details

- **Database:** PostgreSQL 15+ with PostGIS extension.
- **Data Pipeline:** Python 3.11, Pandas, NumPy, Requests.
- **Machine Learning:** Scikit-Learn (Random Forest, GBM), TensorFlow (future LSTM integration).

### Infrastructure

- **Containerization:** Docker, Docker Compose.
- **CI/CD:** GitHub Actions.
- **Hosting (Primary):** Render (Web Services & Static Sites).
- **Hosting (DB):** Supabase (Managed PostgreSQL).

---

## 4. Database Schema

The database is normalized and makes extensive use of PostGIS for spatial operations.

### Core Tables

| Table Name                 | Primary Key | Description                                                                           |
| :------------------------- | :---------- | :------------------------------------------------------------------------------------ |
| `users`                    | `id`        | Application users, includes role-based access control (admin, user).                  |
| `locations`                | `id`        | Monitoring stations. Includes `geom` (PostGIS Point) for spatial indexing.            |
| `water_quality_parameters` | `id`        | Reference table definitions for parameters like BOD, pH, TDS, with safety thresholds. |
| `water_quality_readings`   | `id`        | Time-series data of recorded values linked to `locations` and `parameters`.           |
| `ai_predictions`           | `id`        | Predicted values and confidence scores generated by the ML pipeline.                  |
| `alerts`                   | `id`        | System-generated alerts when readings cross critical thresholds.                      |

### Key Relations

- `water_quality_readings` (Many-to-One) → `locations`
- `water_quality_readings` (Many-to-One) → `water_quality_parameters`
- `ai_predictions` (Many-to-One) → `locations`
- `alerts` (Many-to-One) → `locations`

---

## 5. API Specifications

The API follows RESTful principles. Base URL: `/api`.

### Authentication Endpoints

- `POST /auth/register` - Register a new user account.
- `POST /auth/login` - Authenticate and receive a JWT.
- `GET /auth/me` - Retrieve current user profile (Protected).

### Data Endpoints

- `GET /locations` - List all monitoring stations (supports spatial filtering).
- `GET /locations/:id` - Get details for a specific station.
- `GET /readings/recent` - Get latest readings for all parameters.
- `GET /predictions/risk-map` - Get spatial risk assessment data.
- `GET /alerts/active` - List current critical water quality alerts.

---

## 6. Security Implementation

### Authentication & Authorization

- **JWT Strategy:** Stateless authentication using JSON Web Tokens signed with HS256. Tokens are passed in the `Authorization: Bearer` header.
- **Password Logic:** Passwords are hashed using **bcrypt** with a 10-round salt before storage.
- **Role-Based Access Control (RBAC):** Middleware ensures only 'admin' users can modify reference data or resolve system-wide alerts.

### Data Protection

- **Input Validation:** All incoming request data is validated and sanitized using `express-validator` to prevent injection attacks.
- **Helmet:** Applied to set secure HTTP headers (X-XSS-Protection, X-Frame-Options, Content-Security-Policy).
- **Rate Limiting:** `express-rate-limit` is configured to prevent Brute Force and DDoS attacks on auth routes.
- **CORS:** Strictly configured to allow requests only from the trusted frontend domain.

---

## 7. Deployment & Operations

### CI/CD Pipeline

GitHub Actions orchestrate the deployment workflow:

1. **Lint & Test:** Code analysis (ESLint) and Unit Tests (Jest) run on every push.
2. **Build:** Frontend and Backend assets are built in strict mode.
3. **Deploy:** Upon merger to `main`, services are deployed to Render.
4. **Pipeline:** A scheduled Cron workflow runs the Python Data Pipeline every 30 minutes to fetch fresh data.

### Environment Configuration

The application requires the following environment variables:

- `DATABASE_URL`: Connection string for Supabase PostgreSQL.
- `JWT_SECRET`: Cryptographic key for signing tokens.
- `NODE_ENV`: 'production' or 'development'.
- `PORT`: Service port (default 10000 on Render).
